import logging
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, Union, List, Optional
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from prophet import Prophet
import joblib
import warnings
warnings.filterwarnings("ignore")

# Configura√ß√£o do logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('time_series_processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

warnings.filterwarnings("ignore")

class UnifiedForecaster:
    def __init__(self):
        """Inicializa o modelo com configura√ß√µes otimizadas e valida√ß√µes"""
        # 1. Configura√ß√£o do Prophet
        self.model = Prophet(
            growth='flat',                 # Define o crescimento como 'flat' (constante), ou seja, assume que n√£o h√° crescimento exponencial ao longo do tempo ‚Äî ideal quando se modela s√©ries estacion√°rias como taxas.
            seasonality_mode='additive',  # Define que as sazonalidades ser√£o somadas ao valor da s√©rie (em vez de multiplicadas), o que √© adequado para dados que n√£o variam proporcionalmente √† magnitude do valor base.
            changepoint_prior_scale=0.03,  # Controla a flexibilidade do modelo para se ajustar a mudan√ßas estruturais (changepoints); quanto maior o valor, mais sens√≠vel ele ser√° a mudan√ßas ‚Äî valor 0.3 √© relativamente flex√≠vel.
            interval_width=0.95,          # Define o intervalo de confian√ßa das previs√µes como 95% ‚Äî padr√£o em an√°lises estat√≠sticas para maior seguran√ßa nas proje√ß√µes.
            mcmc_samples=300,             # Ativa a estima√ß√£o bayesiana com amostragem Monte Carlo (MCMC), gerando incerteza mais realista nas previs√µes ‚Äî √∫til para cen√°rios com alta variabilidade como meio ambiente.
            yearly_seasonality=False        # üî¥ Desliga porque os dados s√£o anuais
        ).add_seasonality(
            name='custom',                # Nome dado √† sazonalidade adicional (personalizada).
            period=365.25,                # Define que essa sazonalidade ocorre em ciclos anuais (365.25 dias para incluir ano bissexto).
            fourier_order=1               # Define a complexidade da sazonalidade com 1 termo de Fourier ‚Äî quanto menor o valor, mais suave o padr√£o capturado; evita overfitting.
        )
                
        # 2. Defini√ß√£o das features (X) - todas devem existir ap√≥s o pr√©-processamento
        self.feature_columns = [
            'area_fazenda_ha',     # √Årea agr√≠cola
            'area_floresta_ha',    # √Årea florestal
            'prop_floresta',       # Propor√ß√£o calculada
            'estado_code'          # Estado codificado
        ]
        
        # 3. Metadados geogr√°ficos (n√£o usados como features)
        self.geo_columns = ['Estado', 'bioma']  # Para refer√™ncia p√≥s-predi√ß√£o
        
        # 4. Pr√©-processador
        self.preprocessor = self._create_preprocessor()
        
        # 5. Estado do modelo
        self.is_trained = False
        
        # 6. Codificador de estados
        self.encoder_estados = None  # Ser√° configurado durante o treino

        # Valida√ß√£o interna
        self._validate_init()

    def _validate_init(self):
        """Valida consist√™ncia interna na inicializa√ß√£o"""
        required_features = {'area_fazenda_ha', 'area_floresta_ha', 'prop_floresta', 'estado_code'}
        if not required_features.issubset(set(self.feature_columns)):
            missing = required_features - set(self.feature_columns)
            raise ValueError(f"Features obrigat√≥rias faltando: {missing}")

    def _create_preprocessor(self):
        """Cria pr√©-processador apenas para as features num√©ricas"""
        return ColumnTransformer(
            [('num', StandardScaler(), self.feature_columns)],
            remainder='passthrough',
            verbose_feature_names_out=False  # Nomes originais s√£o mantidos
        )

    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Pr√©-processamento com agrupamento por estado/ano/bioma e c√°lculos corretos"""
        # 1. Valida√ß√£o inicial
        if 'y' not in df.columns:
            raise ValueError("Coluna 'y' n√£o encontrada no DataFrame de entrada")
        
        required_cols = {'ds', 'y', 'Estado', 'area_floresta_ha', 'area_fazenda_ha', 'bioma'}
        if missing := required_cols - set(df.columns):
            raise ValueError(f"Colunas obrigat√≥rias faltando: {missing}")

        # 2. Prepara√ß√£o dos dados
        df = df.copy()
        df['ds'] = pd.to_datetime(df['ds'])
        df['y'] = df['y'].replace([np.inf, -np.inf], np.nan).fillna(0)

        # 3. Agrupamento dos dados
        df_grouped = df.groupby(['Estado', 'ds', 'bioma']).agg({
            'area_fazenda_ha': 'sum',
            'area_floresta_ha': 'sum',
            'y': 'first'  # Usa o valor j√° pr√©-calculado
        }).reset_index()

        # 3.a. Verifica√ß√£o dos valores originais de √°rea
        total_area_original = df_grouped['area_fazenda_ha'] + df_grouped['area_floresta_ha']
        assert not (total_area_original <= 0).any(), "√Årea total inv√°lida detectada"

        # 4. C√°lculo da propor√ß√£o de floresta
        df_grouped['prop_floresta'] = np.where(
            total_area_original > 0,
            df_grouped['area_floresta_ha'] / total_area_original,
            0.0
        )

        # 5. Codifica√ß√£o de estados 
        if self.encoder_estados is None:
            raise RuntimeError("Encoder de estados n√£o configurado - deve ser passado externamente")
        try:
            df_grouped['estado_code'] = self.encoder_estados.transform(df_grouped['Estado'])
        except ValueError as e:
            missing = set(df_grouped['Estado']) - set(self.encoder_estados.classes_)
            raise ValueError(f"Estados n√£o mapeados: {missing}") from e

        # 6. Processamento das features com escalonamento
        processed = df_grouped.copy()
        try:
            features = df_grouped[self.feature_columns]
            if hasattr(self.preprocessor, 'mean_'):
                processed[self.feature_columns] = self.preprocessor.transform(features)
            else:
                processed[self.feature_columns] = self.preprocessor.fit_transform(features)
        except Exception as e:
            raise RuntimeError(f"Falha no escalonamento: {str(e)}")
        
        # A verifica√ß√£o de total de √°rea p√≥s-escalonamento √© removida, pois o StandardScaler pode gerar valores negativos

        # 7. Ordem garantida das colunas
        col_order = [
            'ds', 
            'y', 
            'Estado', 
            'bioma',
            'area_fazenda_ha',
            'area_floresta_ha',
            'prop_floresta',
            'estado_code'
        ] 
        return processed[col_order]

    @staticmethod
    def _aggregate_taxa(series: pd.Series, areas: pd.Series) -> float:
        """Calcula a m√©dia harm√¥nica ponderada das taxas municipais por estado
        Args:
            series: S√©rie com as taxas de convers√£o municipais
            areas: S√©rie com as √°reas totais (floresta + fazenda) de cada munic√≠pio
        """
        # Remove infinitos e NaNs
        valid_mask = series.replace([np.inf, -np.inf], np.nan).notna()
        series = series[valid_mask]
        areas = areas[valid_mask]
        
        if series.empty:
            return 0.0
        
        # Caso especial: se todas as taxas s√£o zero
        if (series == 0).all():
            return 0.0
        
        # M√©dia harm√¥nica ponderada pelas √°reas
        try:
            return 1 / np.average(1 / series, weights=areas)
        except ZeroDivisionError:
            # Fallback para m√©dia harm√¥nica simples se pesos zerados
            return 1 / np.mean(1 / series)

    def train(self, df: pd.DataFrame) -> None:
        """Treina o modelo com valida√ß√µes robustas
        
        Args:
            df: DataFrame com colunas originais (ser√° pr√©-processado)
            
        Raises:
            ValueError: Se dados estiverem inconsistentes
            RuntimeError: Se pr√©-processamento falhar
        """
        try:
            # 1. Pr√©-processamento
            df_processed = self.preprocess_data(df)
            
            # 2. Valida√ß√£o das colunas essenciais
            required_cols = {'ds', 'y'} | set(self.preprocessor.get_feature_names_out())
            missing = required_cols - set(df_processed.columns)
            if missing:
                raise ValueError(f"Colunas faltando ap√≥s pr√©-processamento: {missing}")
            
            # 3. Seleciona APENAS as features definidas em self.feature_columns
            # (evita vazamento de dados ou colunas inv√°lidas)
            feature_cols = [col for col in self.feature_columns 
                        if col in df_processed.columns]
            
            # 4. Adiciona regressores com verifica√ß√£o
            for col in feature_cols:
                if col not in self.model.extra_regressors:  # Evita duplica√ß√£o
                    self.model.add_regressor(col, standardize=True)

            # 5. Treinamento com dataset final
            self.model.fit(df_processed[['ds', 'y'] + feature_cols])
            self.is_trained = True
            
        except Exception as e:
            self.is_trained = False
            raise RuntimeError(f"Falha no treinamento: {str(e)}")

    def predict(self, initial_conditions: Dict, horizon: int = 5) -> pd.DataFrame:
        """Gera previs√µes para um estado espec√≠fico usando condi√ß√µes iniciais.'"""
        if 'last_year' not in initial_conditions:
            raise ValueError("'last_year' √© obrigat√≥rio nas condi√ß√µes iniciais")
        
        try:
            self._validate_inputs(initial_conditions)
            
            if not isinstance(horizon, int) or horizon <= 0:
                raise ValueError("horizon deve ser um inteiro positivo")

            if 'estado_code' not in initial_conditions:
                initial_conditions['estado_code'] = self.encoder_estados.transform([initial_conditions['Estado']])[0]

            if 'prop_floresta' not in initial_conditions:
                total_area = initial_conditions['area_fazenda_ha'] + initial_conditions['area_floresta_ha']
                initial_conditions['prop_floresta'] = (initial_conditions['area_floresta_ha'] / total_area) if total_area > 0 else 0.0

            future = self._create_future_dataframe(initial_conditions, horizon)
            
            future_processed = pd.DataFrame(
                self.preprocessor.transform(future[self.feature_columns]),
                columns=self.feature_columns  # Mant√©m os nomes originais
            )
            # Mantenha as colunas originais para o Prophet:
            future_processed = pd.concat([future[['ds', 'y']], future_processed], axis=1)

            forecast = self.model.predict(future_processed)
            forecast = forecast.join(future[self.geo_columns])

            result = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper', 'Estado', 'bioma']].rename(columns={
                'ds': 'data',
                'yhat': 'conversao_ha_prevista',
                'yhat_lower': 'limite_inferior',
                'yhat_upper': 'limite_superior'
            })

            initial_floresta = initial_conditions['area_floresta_ha']
            result['area_floresta_projetada'] = initial_floresta * (1 - result['conversao_ha_prevista']).cumprod()
            result['area_fazenda_projetada'] = (
                initial_conditions['area_fazenda_ha'] + 
                (initial_floresta - result['area_floresta_projetada'])
            )

            return result[[
                'data', 'Estado', 'bioma',
                'conversao_ha_prevista', 'limite_inferior', 'limite_superior',
                'area_floresta_projetada', 'area_fazenda_projetada'
            ]].round(4)

        except Exception as e:
            logger.error(f"Erro na previs√£o para {initial_conditions.get('Estado', 'Estado desconhecido')}: {str(e)}")
            raise RuntimeError(f"Falha na gera√ß√£o de previs√µes: {str(e)}") from e

    def _validate_inputs(self, inputs: Dict) -> None:
        # 1. Verificar campos obrigat√≥rios
        required_keys = self.feature_columns + self.geo_columns + ['y', 'last_year']
        missing = [k for k in required_keys if k not in inputs]
        if missing:
            raise ValueError(f"Inputs faltando: {missing}")

        # 2. Valida√ß√£o de tipos
        type_checks = {
            'y': (int, float, np.number),
            'area_fazenda_ha': (int, float, np.number),
            'area_floresta_ha': (int, float, np.number),
            'Estado': str,
            'bioma': str,
            'prop_floresta': (int, float, np.number),
            'estado_code': (int, np.integer),
            'last_year': (int, np.integer), 
        }
        
        for key, valid_types in type_checks.items():
            if key in inputs and not isinstance(inputs[key], valid_types):
                raise TypeError(f"{key} deve ser do tipo {valid_types}. Recebido: {type(inputs[key])}")

        # 3. Valida√ß√£o de valores num√©ricos
        numeric_checks = {
            'area_fazenda_ha': (0, None),  # > 0
            'area_floresta_ha': (0, None),  # > 0
            'prop_floresta': (0, 1)        # Entre 0 e 1
            # 'y' pode ser qualquer n√∫mero real (positivo, negativo ou zero)
        }
        
        for key, (min_val, max_val) in numeric_checks.items():
            value = inputs[key]
            if min_val is not None and value < min_val:
                raise ValueError(f"{key} deve ser maior ou igual a {min_val}. Recebido: {value}")
            if max_val is not None and value > max_val:
                raise ValueError(f"{key} deve ser menor ou igual a {max_val}. Recebido: {value}")

        # 4. Valida√ß√£o de bioma
        valid_biomas = ['Amaz√¥nia', 'Caatinga', 'Cerrado', 'Pantanal', 'Mata Atl√¢ntica', 'Pampa']
        if inputs['bioma'] not in valid_biomas:
            raise ValueError(f"Bioma inv√°lido. Op√ß√µes: {valid_biomas}")

        # 5. Valida√ß√£o do estado (se encoder estiver dispon√≠vel)
        if hasattr(self, 'encoder_estados') and hasattr(self.encoder_estados, 'classes_'):
            if inputs['Estado'] not in self.encoder_estados.classes_:
                raise ValueError(f"Estado '{inputs['Estado']}' n√£o foi visto durante o treinamento")

    def _create_future_dataframe(self, initial: Dict, horizon: int) -> pd.DataFrame:
        """Garante continuidade temporal come√ßando no ano seguinte ao √∫ltimo hist√≥rico"""
        # Valida√ß√£o de tipos
        for col in ['area_fazenda_ha', 'area_floresta_ha', 'prop_floresta', 'y']:
            if not isinstance(initial[col], (int, float, np.number)):
                raise TypeError(f"{col} deve ser num√©rico, recebido {type(initial[col])}")

        # Garante que o estado_code est√° correto
        try:
            estado_code = self.encoder_estados.transform([initial['Estado']])[0]
        except ValueError as e:
            raise ValueError(f"Erro ao codificar estado: {str(e)}")
        
        # Pega o √∫ltimo ano dos dados hist√≥ricos
        last_year = initial['last_year']
        
        # Cria datas come√ßando no ano seguinte (2024 se √∫ltimo ano for 2023)
        dates = pd.date_range(
            start=f"{last_year + 1}-01-01",
            periods=horizon + 1,  # +1 para incluir o primeiro ano de proje√ß√£o
            freq='YS'
        )
        dates = dates[:horizon]  # Mant√©m apenas os anos solicitados
        
        # DataFrame base
        df = pd.DataFrame({
            'ds': dates,
            'y': float(initial['y']),
            'area_fazenda_ha': float(initial['area_fazenda_ha']),
            'area_floresta_ha': float(initial['area_floresta_ha']),
            'prop_floresta': float(initial['prop_floresta']),
            'estado_code': int(initial['estado_code']),
            'Estado': initial['Estado'],
            'bioma': initial['bioma']
        })

        return df


    def _format_output(self, forecast: pd.DataFrame, initial: Dict) -> pd.DataFrame:
        """Formata os resultados da previs√£o """
        result = forecast[[
            'ds', 'yhat', 'yhat_lower', 'yhat_upper',
            'Estado', 'bioma'  # Mant√©m apenas estado e bioma
        ]].rename(columns={
            'ds': 'data',
            'yhat': 'conversao_ha_prevista',
            'yhat_lower': 'limite_inferior',
            'yhat_upper': 'limite_superior'
        })
        
        # Calcular m√©tricas derivadas
        initial_floresta = initial['area_floresta_ha']
        result['area_floresta_projetada'] = initial_floresta * (1 - initial['y'])**(result.index + 1)  # Usar 'y'
        result['area_fazenda_projetada'] = initial['area_fazenda_ha'] + (initial_floresta - result['area_floresta_projetada'])
        
        # Ordenar colunas
        return result[[
            'data', 'Estado', 'bioma',
            'conversao_ha_prevista', 'limite_inferior', 'limite_superior',
            'area_floresta_projetada', 'area_fazenda_projetada'
        ]].round(2)

    def save_model(self, output_path: Union[str, Path]) -> None:
        """Salva o modelo com verifica√ß√£o de integridade"""
        if not self.is_trained:
            raise RuntimeError("Modelo n√£o treinado")
            
        required_artifacts = ['model', 'preprocessor', 'encoder_estados']
        for artifact in required_artifacts:
            if not hasattr(self, artifact) or getattr(self, artifact) is None:
                raise ValueError(f"Artefato {artifact} n√£o est√° dispon√≠vel para salvar")

        try:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            joblib.dump({
                'model': self.model,
                'preprocessor': self.preprocessor,
                'encoder_estados': self.encoder_estados,
                'feature_columns': self.feature_columns,
                'geo_columns': self.geo_columns,
                'metadata': {
                    'train_date': pd.Timestamp.now(),
                    'states_trained': list(self.encoder_estados.classes_)
                }
            }, output_path)
            
        except Exception as e:
            logger.error(f"Erro ao salvar modelo: {str(e)}")
            raise

    @classmethod
    def load_model(cls, model_path: Union[str, Path]) -> 'UnifiedForecaster':
        """Carrega um modelo salvo garantindo que o pr√©-processador est√° ajustado"""
        try:
            # Carrega os artefatos
            artifacts = joblib.load(model_path)
        
            # Cria uma nova inst√¢ncia do forecaster
            forecaster = cls()
            
            # Restaura os componentes
            forecaster.model = artifacts.get('model')
            forecaster.preprocessor = artifacts.get('preprocessor')
            forecaster.feature_columns = artifacts.get('feature_columns', [])
            forecaster.geo_columns = artifacts.get('geo_columns', [])
            # Recuperar o encoder
            forecaster.encoder_estados = artifacts.get('encoder_estados')

            # Verifica se o encoder_estados est√° presente
            if forecaster.encoder_estados is None:
                raise RuntimeError("Encoder de estados n√£o encontrado")

            # Verifica se o pr√©-processador est√° ajustado
            if hasattr(forecaster.preprocessor, 'transform'):
                # Se j√° estiver fitted, apenas retorna
                forecaster.is_trained = True
            else:
                raise RuntimeError("Pr√©-processador n√£o foi ajustado corretamente")
                
            return forecaster
        except FileNotFoundError:
            raise FileNotFoundError(f"Modelo n√£o encontrado em: {model_path}")
        except Exception as e:
            raise RuntimeError(f"Erro ao carregar o modelo: {e}")
        
    def get_state_historical_data(self, df: pd.DataFrame, estado: str) -> Dict[str, float]:
        """Extrai m√©dias hist√≥ricas de um estado espec√≠fico para usar como condi√ß√µes iniciais"""
        # Mapeamento de colunas alternativas
        col_y = 'y' if 'y' in df.columns else 'taxa_conversao_anual'
        
        required_cols = {'Estado', 'bioma', 'area_fazenda_ha', 'area_floresta_ha', col_y}
        if missing := required_cols - set(df.columns):
            raise ValueError(f"Colunas faltando no DataFrame: {missing}")

        state_data = df[df['Estado'] == estado]
        
        if state_data.empty:
            raise ValueError(f"Dados n√£o encontrados para o estado: {estado}")
        
        # Calcula propor√ß√£o m√©dia de floresta
        total_area = state_data['area_fazenda_ha'] + state_data['area_floresta_ha']
        prop_floresta = (state_data['area_floresta_ha'] / total_area).mean()
        
        return {
            'Estado': estado,
            'bioma': state_data['bioma'].iloc[0],
            'area_fazenda_ha': state_data['area_fazenda_ha'].mean(),
            'area_floresta_ha': state_data['area_floresta_ha'].mean(),
            'prop_floresta': prop_floresta,
            'y': state_data[col_y].mean(),  # Alterado para 'y'
            'estado_code': state_data['estado_code'].iloc[0] if 'estado_code' in state_data.columns else 0
        }